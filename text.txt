================================================================================
Experiment 1: Deep Learning Frameworks & Ethics
================================================================================

Aim: Introduction to Deep Learning Frameworks and Ethical AI Considerations

Theory:
Deep learning frameworks like TensorFlow and PyTorch provide comprehensive toolkits for neural networks with automatic differentiation, GPU acceleration, and pre-trained models. Ethical AI encompasses fairness, transparency, and privacy preservation in AI systems.

Implementation:
---------------
!pip install tensorflow torch pandas

import tensorflow as tf
import pandas as pd
import numpy as np

# Load custom dataset (replace with your CSV file path)
def load_custom_data(csv_path='data.csv'):
    try:
        df = pd.read_csv(csv_path)
        X = df.drop(columns=['target']).values  # Assumes 'target' column exists
        y = df['target'].values
        return X, y
    except:
        # Fallback: Generate synthetic data
        X = np.random.random((1000, 10))
        y = np.random.randint(0, 2, 1000)
        return X, y

# Load data
X, y = load_custom_data()

# Build simple neural network
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

Visualization:
--------------
import matplotlib.pyplot as plt

model.summary()
print(f"TensorFlow: {tf.__version__}")

# Optional architecture plot
try:
    tf.keras.utils.plot_model(model, show_shapes=True)
except:
    print("Model visualization requires graphviz")


================================================================================
Experiment 2: Deep Neural Network for Multi-class Classification
================================================================================

Aim: Build Deep Neural Network with Multiple Hidden Layers for Classification

Theory:
Deep Neural Networks use multiple layers for hierarchical feature learning. ReLU activation prevents vanishing gradients, while softmax enables multi-class probability distribution.

Implementation:
---------------
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load custom CSV data
def load_classification_data(csv_path='classification_data.csv'):
    try:
        df = pd.read_csv(csv_path)
        X = df.drop(columns=['label']).values
        y = df['label'].values
        
        # Encode labels if categorical
        le = LabelEncoder()
        y = le.fit_transform(y)
        num_classes = len(np.unique(y))
        
        return X, y, num_classes
    except:
        # Fallback: MNIST for demonstration
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
        X = X_train.reshape(-1, 784) / 255.0
        y = y_train
        return X, y, 10

# Load and preprocess data
X, y, num_classes = load_classification_data()

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# One-hot encode labels
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)
y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)

# Build DNN model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(X.shape[1],)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train_cat, epochs=20, batch_size=128, 
                   validation_split=0.2, verbose=1)

# Evaluate
test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"Test Accuracy: {test_acc*100:.2f}%")

Visualization:
--------------
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()


================================================================================
Experiment 3: Object Detection Using Pre-trained SSD MobileNetV2
================================================================================

Aim: Implement Object Detection using Pre-trained Models

Theory:
Object detection combines classification and localization. SSD performs single-shot detection with multi-scale feature maps, while MobileNetV2 provides efficient feature extraction.

Implementation:
---------------
import tensorflow as tf
import tensorflow_hub as hub
import pandas as pd
import cv2
import numpy as np
from PIL import Image

# Load image paths from CSV
def load_image_data(csv_path='image_data.csv'):
    try:
        df = pd.read_csv(csv_path)
        image_paths = df['image_path'].tolist()
        return image_paths
    except:
        # Fallback: download sample image
        import urllib.request
        url = 'https://images.cocodataset.org/val2017/000000039769.jpg'
        urllib.request.urlretrieve(url, 'sample.jpg')
        return ['sample.jpg']

# Load detection model
detector = hub.load("https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1")

# Load image paths
image_paths = load_image_data()

# Process first image
image_path = image_paths[0]
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image_resized = cv2.resize(image_rgb, (640, 640))

# Prepare input tensor
input_tensor = tf.convert_to_tensor(image_resized, dtype=tf.uint8)
input_tensor = tf.expand_dims(input_tensor, 0)

# Run detection
detections = detector(input_tensor)

print(f"Detected {len(detections['detection_boxes'][0])} objects")

Visualization:
--------------
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# COCO class names (subset)
class_names = {1: 'person', 2: 'bicycle', 3: 'car', 16: 'bird', 17: 'cat', 18: 'dog'}

plt.figure(figsize=(12, 8))
plt.imshow(image_rgb)
ax = plt.gca()

boxes = detections['detection_boxes'][0].numpy()
classes = detections['detection_classes'][0].numpy().astype(int)
scores = detections['detection_scores'][0].numpy()

threshold = 0.3
for i in range(len(boxes)):
    if scores[i] > threshold:
        ymin, xmin, ymax, xmax = boxes[i]
        h, w = image_rgb.shape[:2]
        left, top, right, bottom = int(xmin*w), int(ymin*h), int(xmax*w), int(ymax*h)
        
        rect = patches.Rectangle((left, top), right-left, bottom-top,
                               linewidth=2, edgecolor='red', facecolor='none')
        ax.add_patch(rect)
        
        class_name = class_names.get(classes[i], f'Class {classes[i]}')
        ax.text(left, top-10, f'{class_name}: {scores[i]:.2f}',
                color='white', fontweight='bold',
                bbox=dict(facecolor='red', alpha=0.7))

plt.title('Object Detection Results')
plt.axis('off')
plt.show()


================================================================================
Experiment 4: Semantic Segmentation with DeepLabV3
================================================================================

Aim: Perform Pixel-level Semantic Segmentation

Theory:
Semantic segmentation assigns class labels to every pixel. DeepLabV3 uses atrous convolutions for multi-scale context and ASPP for feature sampling.

Implementation:
---------------
from transformers import MobileNetV2ImageProcessor, MobileNetV2ForSemanticSegmentation
import pandas as pd
import torch
from PIL import Image

# Load image paths from CSV
def load_segmentation_data(csv_path='segmentation_data.csv'):
    try:
        df = pd.read_csv(csv_path)
        return df['image_path'].tolist()
    except:
        # Create sample image
        import numpy as np
        sample = np.random.randint(0, 255, (300, 400, 3), dtype=np.uint8)
        return [Image.fromarray(sample)]

# Load model
processor = MobileNetV2ImageProcessor.from_pretrained("Matthijs/deeplabv3-mobilenet-v2")
model = MobileNetV2ForSemanticSegmentation.from_pretrained("Matthijs/deeplabv3-mobilenet-v2")

# Load images
image_paths = load_segmentation_data()
image = Image.open(image_paths[0]).convert('RGB') if isinstance(image_paths[0], str) else image_paths[0]

# Process image
inputs = processor(images=image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

mask = outputs.logits.argmax(dim=1)[0].detach().cpu().numpy()
print(f"Segmentation mask shape: {mask.shape}")
print(f"Unique classes: {np.unique(mask)}")

Visualization:
--------------
import matplotlib.pyplot as plt

# PASCAL VOC classes
classes = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',
          'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',
          'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa',
          'train', 'tvmonitor']

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.imshow(image)
plt.title('Original Image')
plt.axis('off')

plt.subplot(1, 3, 2)
plt.imshow(mask, cmap='tab20')
plt.title('Segmentation Mask')
plt.axis('off')

plt.subplot(1, 3, 3)
unique_classes, counts = np.unique(mask, return_counts=True)
class_names = [classes[i] if i < len(classes) else f'Class {i}' for i in unique_classes]
plt.bar(range(len(unique_classes)), counts)
plt.xticks(range(len(unique_classes)), class_names, rotation=45)
plt.title('Class Distribution')
plt.ylabel('Pixel Count')

plt.tight_layout()
plt.show()


================================================================================
Experiment 5: Autoencoder for Data Compression
================================================================================

Aim: Implement Autoencoder for Dimensionality Reduction

Theory:
Autoencoders learn compressed representations through encoder-decoder architecture. The bottleneck forces learning of essential features for reconstruction.

Implementation:
---------------
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Load custom data
def load_autoencoder_data(csv_path='autoencoder_data.csv'):
    try:
        df = pd.read_csv(csv_path)
        return df.values
    except:
        # Fallback: MNIST
        (x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
        x_train = x_train.reshape(-1, 784) / 255.0
        x_test = x_test.reshape(-1, 784) / 255.0
        return x_train, x_test

# Load data
try:
    data = load_autoencoder_data()
    if isinstance(data, tuple):
        x_train, x_test = data
    else:
        from sklearn.model_selection import train_test_split
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data)
        x_train, x_test = train_test_split(data_scaled, test_size=0.2, random_state=42)
except:
    (x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
    x_train = x_train.reshape(-1, 784) / 255.0
    x_test = x_test.reshape(-1, 784) / 255.0

input_dim = x_train.shape[1]
encoding_dim = min(32, input_dim // 4)  # Adaptive encoding dimension

# Build autoencoder
encoder = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(encoding_dim, activation='relu')
])

decoder = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(encoding_dim,)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(input_dim, activation='sigmoid')
])

autoencoder = tf.keras.Sequential([encoder, decoder])
autoencoder.compile(optimizer='adam', loss='mse')

# Train
history = autoencoder.fit(x_train, x_train, epochs=50, batch_size=256,
                         validation_data=(x_test, x_test), verbose=1)

print(f"Compression ratio: {input_dim/encoding_dim:.1f}:1")

Visualization:
--------------
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))

# Loss curves
plt.subplot(2, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('MSE')
plt.legend()
plt.grid(True)

# Reconstruction comparison
plt.subplot(2, 2, 2)
reconstructed = autoencoder.predict(x_test[:5])
for i in range(5):
    plt.subplot(2, 5, i+6)
    if input_dim == 784:  # MNIST case
        plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
        plt.title(f'Original {i+1}')
        plt.axis('off')
        
        plt.subplot(2, 5, i+11)
        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')
        plt.title(f'Reconstructed {i+1}')
        plt.axis('off')

plt.tight_layout()
plt.show()


================================================================================
Experiment 6: Denoising Autoencoder
================================================================================

Aim: Build Denoising Autoencoder for Noise Removal

Theory:
Denoising autoencoders learn robust representations by training on corrupted inputs while targeting clean outputs, improving generalization.

Implementation:
---------------
import tensorflow as tf
import pandas as pd
import numpy as np

# Load data with noise injection
def load_noisy_data(csv_path='noisy_data.csv', noise_factor=0.3):
    try:
        df = pd.read_csv(csv_path)
        clean_data = df.values
        noisy_data = clean_data + noise_factor * np.random.normal(size=clean_data.shape)
        noisy_data = np.clip(noisy_data, 0, 1)
        return clean_data, noisy_data
    except:
        # Fallback: MNIST with synthetic noise
        (x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
        x_train = x_train.reshape(-1, 28, 28, 1) / 255.0
        x_test = x_test.reshape(-1, 28, 28, 1) / 255.0
        
        x_train_noisy = x_train + noise_factor * np.random.normal(size=x_train.shape)
        x_test_noisy = x_test + noise_factor * np.random.normal(size=x_test.shape)
        
        x_train_noisy = np.clip(x_train_noisy, 0, 1)
        x_test_noisy = np.clip(x_test_noisy, 0, 1)
        
        return (x_train, x_test), (x_train_noisy, x_test_noisy)

# Load data
clean_data, noisy_data = load_noisy_data()

if isinstance(clean_data, tuple):
    x_train, x_test = clean_data
    x_train_noisy, x_test_noisy = noisy_data
else:
    from sklearn.model_selection import train_test_split
    x_train, x_test = train_test_split(clean_data, test_size=0.2, random_state=42)
    x_train_noisy, x_test_noisy = train_test_split(noisy_data, test_size=0.2, random_state=42)

# Build denoising autoencoder
input_shape = x_train.shape[1:]
input_img = tf.keras.Input(shape=input_shape)

if len(input_shape) == 3:  # Image data
    # Convolutional denoising autoencoder
    encoded = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(input_img)
    encoded = tf.keras.layers.MaxPooling2D((2,2), padding='same')(encoded)
    encoded = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(encoded)
    encoded = tf.keras.layers.MaxPooling2D((2,2), padding='same')(encoded)
    
    decoded = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(encoded)
    decoded = tf.keras.layers.UpSampling2D((2,2))(decoded)
    decoded = tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same')(decoded)
    decoded = tf.keras.layers.UpSampling2D((2,2))(decoded)
    decoded = tf.keras.layers.Conv2D(1, (3,3), activation='sigmoid', padding='same')(decoded)
else:  # Tabular data
    # Dense denoising autoencoder
    encoded = tf.keras.layers.Dense(128, activation='relu')(input_img)
    encoded = tf.keras.layers.Dense(64, activation='relu')(encoded)
    encoded = tf.keras.layers.Dense(32, activation='relu')(encoded)
    
    decoded = tf.keras.layers.Dense(64, activation='relu')(encoded)
    decoded = tf.keras.layers.Dense(128, activation='relu')(decoded)
    decoded = tf.keras.layers.Dense(input_shape[0], activation='sigmoid')(decoded)

denoising_autoencoder = tf.keras.Model(input_img, decoded)
denoising_autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

# Train
history = denoising_autoencoder.fit(
    x_train_noisy, x_train,
    epochs=30, batch_size=128,
    validation_data=(x_test_noisy, x_test),
    verbose=1
)

Visualization:
--------------
import matplotlib.pyplot as plt

# Training history
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Denoising Training')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Denoising results
denoised = denoising_autoencoder.predict(x_test_noisy[:5])

if len(input_shape) == 3:  # Image visualization
    for i in range(5):
        plt.subplot(3, 5, i+1)
        plt.imshow(x_test[i].squeeze(), cmap='gray')
        plt.title(f'Clean {i+1}')
        plt.axis('off')
        
        plt.subplot(3, 5, i+6)
        plt.imshow(x_test_noisy[i].squeeze(), cmap='gray')
        plt.title(f'Noisy {i+1}')
        plt.axis('off')
        
        plt.subplot(3, 5, i+11)
        plt.imshow(denoised[i].squeeze(), cmap='gray')
        plt.title(f'Denoised {i+1}')
        plt.axis('off')

plt.tight_layout()
plt.show()


================================================================================
Experiment 7: Variational Autoencoder (VAE)
================================================================================

Aim: Implement VAE for Generative Modeling

Theory:
VAEs learn probabilistic latent representations using reparameterization trick. They enable data generation through latent space sampling with KL divergence regularization.

Implementation:
---------------
import tensorflow as tf
import pandas as pd
import numpy as np

# Load data
def load_vae_data(csv_path='vae_data.csv'):
    try:
        df = pd.read_csv(csv_path)
        data = df.values
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        return scaler.fit_transform(data)
    except:
        (x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()
        x_train = x_train.reshape(-1, 784) / 255.0
        return x_train

x_train = load_vae_data()
input_dim = x_train.shape[1]
latent_dim = 2

# Sampling layer
class Sampling(tf.keras.layers.Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# Encoder
encoder_inputs = tf.keras.Input(shape=(input_dim,))
x = tf.keras.layers.Dense(256, activation='relu')(encoder_inputs)
x = tf.keras.layers.Dense(128, activation='relu')(x)
z_mean = tf.keras.layers.Dense(latent_dim)(x)
z_log_var = tf.keras.layers.Dense(latent_dim)(x)
z = Sampling()([z_mean, z_log_var])
encoder = tf.keras.Model(encoder_inputs, [z_mean, z_log_var, z])

# Decoder
latent_inputs = tf.keras.Input(shape=(latent_dim,))
x = tf.keras.layers.Dense(128, activation='relu')(latent_inputs)
x = tf.keras.layers.Dense(256, activation='relu')(x)
decoder_outputs = tf.keras.layers.Dense(input_dim, activation='sigmoid')(x)
decoder = tf.keras.Model(latent_inputs, decoder_outputs)

# VAE class
class VAE(tf.keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super().__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        
    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)
            
            reconstruction_loss = tf.reduce_mean(
                tf.keras.losses.binary_crossentropy(data, reconstruction)
            )
            kl_loss = -0.5 * tf.reduce_mean(
                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            )
            total_loss = reconstruction_loss + kl_loss
            
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {"loss": total_loss, "reconstruction_loss": reconstruction_loss, "kl_loss": kl_loss}

# Train VAE
vae = VAE(encoder, decoder)
vae.compile(optimizer='adam')
history = vae.fit(x_train, epochs=50, batch_size=128)

Visualization:
--------------
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

# Latent space
plt.subplot(1, 3, 1)
z_mean, _, _ = encoder.predict(x_train[:1000])
plt.scatter(z_mean[:, 0], z_mean[:, 1], alpha=0.6)
plt.title('Latent Space')
plt.xlabel('Latent Dim 1')
plt.ylabel('Latent Dim 2')

# Generated samples
plt.subplot(1, 3, 2)
n = 15
figure = np.zeros((28 * n, 28 * n))
grid_x = np.linspace(-3, 3, n)
grid_y = np.linspace(-3, 3, n)

if input_dim == 784:  # MNIST case
    for i, yi in enumerate(grid_x):
        for j, xi in enumerate(grid_y):
            z_sample = np.array([[xi, yi]])
            x_decoded = decoder.predict(z_sample)
            digit = x_decoded[0].reshape(28, 28)
            figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit
    
    plt.imshow(figure, cmap='Greys_r')
    plt.title('Generated Samples')
    plt.axis('off')

plt.tight_layout()
plt.show()


================================================================================
Experiment 8: CNN for Image Classification
================================================================================

Aim: Implement CNN for Image Classification

Theory:
CNNs use convolutional layers for spatial feature extraction, pooling for dimensionality reduction, and fully connected layers for classification. They excel in image recognition tasks.

Implementation:
---------------
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report

# Load image data
def load_cnn_data(csv_path='cnn_data.csv'):
    try:
        df = pd.read_csv(csv_path)
        # Assumes CSV has image_path and label columns
        images = []
        labels = []
        for _, row in df.iterrows():
            img = tf.keras.preprocessing.image.load_img(row['image_path'], target_size=(28, 28))
            img_array = tf.keras.preprocessing.image.img_to_array(img)
            images.append(img_array)
            labels.append(row['label'])
        
        X = np.array(images) / 255.0
        y = np.array(labels)
        return X, y
    except:
        # Fallback: MNIST
        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
        x_train = x_train.reshape(-1, 28, 28, 1) / 255.0
        x_test = x_test.reshape(-1, 28, 28, 1) / 255.0
        return (x_train, y_train), (x_test, y_test)

# Load data
data = load_cnn_data()
if isinstance(data[0], tuple):
    (x_train, y_train), (x_test, y_test) = data
else:
    X, y = data
    from sklearn.model_selection import train_test_split
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

num_classes = len(np.unique(y_train))
y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes)
y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)

# Build CNN
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=x_train.shape[1:]),
    tf.keras.layers.MaxPooling2D((2,2)),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2,2)),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train with callbacks
callbacks = [
    tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
    tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)
]

history = model.fit(x_train, y_train_cat, epochs=25, batch_size=128,
                   validation_split=0.2, callbacks=callbacks, verbose=1)

# Evaluate
test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=0)
print(f"Test Accuracy: {test_acc*100:.2f}%")

Visualization:
--------------
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

plt.figure(figsize=(15, 10))

# Training history
plt.subplot(2, 3, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(2, 3, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Confusion matrix
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)

plt.subplot(2, 3, 3)
cm = confusion_matrix(y_test, y_pred_classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Sample predictions
plt.subplot(2, 3, 4)
sample_indices = np.random.choice(len(x_test), 6)
fig, axes = plt.subplots(2, 3, figsize=(8, 6))
for i, idx in enumerate(sample_indices):
    ax = axes[i//3, i%3]
    ax.imshow(x_test[idx].squeeze(), cmap='gray')
    pred_class = y_pred_classes[idx]
    true_class = y_test[idx]
    confidence = np.max(y_pred[idx])
    color = 'green' if pred_class == true_class else 'red'
    ax.set_title(f'T:{true_class} P:{pred_class}\nConf:{confidence:.3f}', color=color)
    ax.axis('off')

plt.tight_layout()
plt.show()

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred_classes))
