================================================================================
Experiment 1: Deep Learning Frameworks & Ethics
================================================================================

Aim: Introduction to Deep Learning Frameworks and Ethical AI Considerations

Theory:
Deep learning frameworks like TensorFlow and PyTorch have revolutionized artificial intelligence development by providing comprehensive toolkits for building, training, and deploying neural networks. These frameworks offer automatic differentiation, GPU acceleration, high-level APIs, and extensive pre-trained model libraries that enable rapid prototyping and production deployment.

TensorFlow, developed by Google, emphasizes production-ready deployments with TensorFlow Serving and TensorFlow Lite for mobile devices. PyTorch, created by Facebook, focuses on research flexibility with dynamic computation graphs and intuitive debugging capabilities.

Ethical AI encompasses fairness, transparency, accountability, and privacy preservation. Key considerations include algorithmic bias mitigation, explainable AI for decision transparency, data privacy protection, and ensuring AI systems benefit all users equitably without discrimination.

Implementation:
---------------
# Install necessary frameworks (execute once)
!pip install tensorflow torch

import tensorflow as tf
import torch

# Build a simple feedforward neural network using TensorFlow/Keras
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile model with Adam optimizer and binary crossentropy loss
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

print("Model created successfully!")
print("Framework versions:")
print(f"TensorFlow: {tf.__version__}")
print(f"PyTorch: {torch.__version__}")

Visualization:
--------------
# Display model architecture and summary
model.summary()

# Optional: Plot model architecture (requires pydot and graphviz)
try:
    from tensorflow.keras.utils import plot_model
    plot_model(model, to_file='model_architecture.png', 
               show_shapes=True, show_layer_names=True)
    print("Model architecture saved as 'model_architecture.png'")
except:
    print("Model visualization requires pydot and graphviz installation")


================================================================================
Experiment 2: Deep Neural Network for Multi-class Classification
================================================================================

Aim: Build and Train a Deep Neural Network with Multiple Hidden Layers for MNIST Digit Classification

Theory:
Deep Neural Networks (DNNs) consist of multiple hidden layers that enable learning of hierarchical feature representations. Each layer progressively extracts more abstract features from raw input data. The universal approximation theorem states that neural networks with sufficient hidden units can approximate any continuous function.

Key architectural components include:
- Input layer: Receives raw data (784 features for flattened 28x28 MNIST images)
- Hidden layers: Apply linear transformations followed by non-linear activations (ReLU prevents vanishing gradients)
- Output layer: Uses softmax activation for multi-class probability distribution

Backpropagation algorithm enables training by computing gradients of loss function with respect to network weights, allowing optimization via gradient descent variants like Adam.

Implementation:
---------------
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load MNIST handwritten digit dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Data preprocessing
print(f"Original training data shape: {X_train.shape}")
print(f"Original test data shape: {X_test.shape}")

# Flatten 28x28 images to 784-dimensional vectors and normalize
X_train = X_train.reshape(-1, 784).astype('float32') / 255.0
X_test = X_test.reshape(-1, 784).astype('float32') / 255.0

# Convert labels to one-hot encoded vectors
y_train_categorical = to_categorical(y_train, 10)
y_test_categorical = to_categorical(y_test, 10)

print(f"Processed training data shape: {X_train.shape}")
print(f"One-hot encoded labels shape: {y_train_categorical.shape}")

# Build deep neural network with two hidden layers
dnn_model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),  # Regularization to prevent overfitting
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')  # 10 classes for digits 0-9
])

# Compile model with categorical crossentropy for multi-class classification
dnn_model.compile(optimizer='adam',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

# Train model with validation split
print("Training deep neural network...")
training_history = dnn_model.fit(
    X_train, y_train_categorical,
    epochs=15,
    batch_size=128,
    validation_split=0.1,
    verbose=1
)

# Evaluate on test data
test_loss, test_accuracy = dnn_model.evaluate(X_test, y_test_categorical, verbose=0)
print(f"Test Accuracy: {test_accuracy*100:.2f}%")

Visualization:
--------------
import matplotlib.pyplot as plt

# Plot training history
plt.figure(figsize=(15, 5))

# Accuracy plot
plt.subplot(1, 3, 1)
plt.plot(training_history.history['accuracy'], label='Training Accuracy', linewidth=2)
plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Loss plot
plt.subplot(1, 3, 2)
plt.plot(training_history.history['loss'], label='Training Loss', linewidth=2)
plt.plot(training_history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Sample predictions visualization
plt.subplot(1, 3, 3)
predictions = dnn_model.predict(X_test[:10])
predicted_classes = np.argmax(predictions, axis=1)
actual_classes = y_test[:10]

plt.bar(range(10), predicted_classes, alpha=0.7, label='Predicted')
plt.bar(range(10), actual_classes, alpha=0.5, label='Actual')
plt.title('Sample Predictions vs Actual')
plt.xlabel('Sample Index')
plt.ylabel('Digit Class')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()


================================================================================
Experiment 3: Object Detection Using TensorFlow Hub SSD MobileNetV2
================================================================================

Aim: Implement Object Detection using Pre-trained SSD MobileNetV2 Model for Real-time Object Identification

Theory:
Object detection combines classification and localization tasks to identify multiple objects within images and determine their precise locations using bounding boxes. Single Shot Detector (SSD) architecture performs detection in a single forward pass, making it suitable for real-time applications.

SSD key features:
- Multi-scale feature maps: Detects objects at different scales
- Default boxes: Pre-defined anchor boxes at various aspect ratios
- Non-maximum suppression: Eliminates duplicate detections

MobileNetV2 backbone provides efficient feature extraction optimized for mobile and embedded devices using depthwise separable convolutions that reduce computational complexity while maintaining accuracy.

TensorFlow Hub offers pre-trained models trained on large datasets like COCO, enabling transfer learning without extensive training time.

Implementation:
---------------
import tensorflow as tf
import tensorflow_hub as hub
import cv2
import numpy as np
import urllib.request
from PIL import Image

# Download sample COCO dataset image for testing
print("Downloading test image...")
urllib.request.urlretrieve(
    'https://images.cocodataset.org/val2017/000000039769.jpg', 
    'sample_detection_image.jpg'
)

# Load pre-trained SSD MobileNetV2 model from TensorFlow Hub
print("Loading SSD MobileNetV2 model from TensorFlow Hub...")
ssd_detector = hub.load("https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1")

# Read and preprocess input image
input_image = cv2.imread('sample_detection_image.jpg')
image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)
image_height, image_width = image_rgb.shape[:2]

# Resize image to model input requirements
image_resized = cv2.resize(image_rgb, (640, 640))

# Convert to tensor format expected by model
input_tensor = tf.convert_to_tensor(image_resized, dtype=tf.uint8)
input_tensor = tf.expand_dims(input_tensor, 0)  # Add batch dimension

print("Running object detection...")
# Perform object detection
detection_results = ssd_detector(input_tensor)

# Extract detection results
detection_boxes = detection_results['detection_boxes'][0].numpy()
detection_classes = detection_results['detection_classes'][0].numpy().astype(int)
detection_scores = detection_results['detection_scores'][0].numpy()

print(f"Detected {len(detection_boxes)} potential objects")
print(f"Detection scores range: {detection_scores.min():.3f} to {detection_scores.max():.3f}")

Visualization:
--------------
import matplotlib.pyplot as plt
import matplotlib.patches as patches

# COCO dataset class names (subset for demonstration)
coco_class_names = {
    1: 'person', 2: 'bicycle', 3: 'car', 4: 'motorcycle', 5: 'airplane',
    6: 'bus', 7: 'train', 8: 'truck', 9: 'boat', 10: 'traffic light',
    11: 'fire hydrant', 13: 'stop sign', 14: 'parking meter', 15: 'bench',
    16: 'bird', 17: 'cat', 18: 'dog', 19: 'horse', 20: 'sheep',
    21: 'cow', 22: 'elephant', 23: 'bear', 24: 'zebra', 25: 'giraffe'
}

# Set confidence threshold for displaying detections
confidence_threshold = 0.3

plt.figure(figsize=(15, 10))
plt.imshow(image_rgb)
ax = plt.gca()

detected_objects = []
for i in range(len(detection_boxes)):
    if detection_scores[i] >= confidence_threshold:
        # Convert normalized coordinates to pixel coordinates
        ymin, xmin, ymax, xmax = detection_boxes[i]
        left = int(xmin * image_width)
        top = int(ymin * image_height)
        right = int(xmax * image_width)
        bottom = int(ymax * image_height)
        
        # Draw bounding box
        rect = patches.Rectangle(
            (left, top), right - left, bottom - top,
            linewidth=3, edgecolor='red', facecolor='none'
        )
        ax.add_patch(rect)
        
        # Add class label and confidence score
        class_name = coco_class_names.get(detection_classes[i], f'Class {detection_classes[i]}')
        label = f'{class_name}: {detection_scores[i]:.2f}'
        ax.text(left, top - 10, label, 
                color='white', fontsize=12, fontweight='bold',
                bbox=dict(facecolor='red', alpha=0.8))
        
        detected_objects.append((class_name, detection_scores[i]))

plt.title(f'Object Detection Results (Threshold: {confidence_threshold})', fontsize=16)
plt.axis('off')
plt.show()

# Print detection summary
print(f"\nDetected objects above {confidence_threshold} confidence:")
for obj, score in detected_objects:
    print(f"  {obj}: {score:.3f}")


================================================================================
Experiment 4: Semantic Image Segmentation with DeepLabV3 + MobileNetV2
================================================================================

Aim: Perform Pixel-level Semantic Segmentation using DeepLabV3 Architecture with MobileNetV2 Backbone

Theory:
Semantic segmentation assigns class labels to every pixel in an image, providing dense pixel-wise predictions for scene understanding. Unlike object detection which uses bounding boxes, segmentation provides precise object boundaries.

DeepLabV3 key innovations:
- Atrous (dilated) convolutions: Capture multi-scale context without losing resolution
- Atrous Spatial Pyramid Pooling (ASPP): Samples features at multiple scales
- Skip connections: Preserve fine-grained spatial information

MobileNetV2 backbone uses inverted residuals and linear bottlenecks for efficient feature extraction suitable for mobile deployment while maintaining segmentation accuracy.

Applications include autonomous driving (road/pedestrian segmentation), medical imaging (organ segmentation), and augmented reality (background replacement).

Implementation:
---------------
from transformers import MobileNetV2ImageProcessor, MobileNetV2ForSemanticSegmentation
from PIL import Image
import torch
import numpy as np
import matplotlib.pyplot as plt

# Load sample image for segmentation (provide your own image path)
image_path = 'segmentation_test_image.jpg'  # Update with actual image path

try:
    input_image = Image.open(image_path).convert('RGB')
    print(f"Loaded image with size: {input_image.size}")
except FileNotFoundError:
    print("Creating sample image for demonstration...")
    # Create a simple sample image if no image file exists
    sample_image = np.random.randint(0, 255, (300, 400, 3), dtype=np.uint8)
    input_image = Image.fromarray(sample_image)

# Load pre-trained DeepLabV3 model with MobileNetV2 backbone
print("Loading DeepLabV3 + MobileNetV2 model from Hugging Face...")
processor = MobileNetV2ImageProcessor.from_pretrained("Matthijs/deeplabv3-mobilenet-v2")
segmentation_model = MobileNetV2ForSemanticSegmentation.from_pretrained("Matthijs/deeplabv3-mobilenet-v2")

# Preprocess image for model input
model_inputs = processor(images=input_image, return_tensors="pt")

print("Performing semantic segmentation...")
# Run segmentation inference
with torch.no_grad():
    model_outputs = segmentation_model(**model_inputs)

# Extract segmentation mask
segmentation_logits = model_outputs.logits
segmentation_mask = segmentation_logits.argmax(dim=1)[0].detach().cpu().numpy()

print(f"Segmentation mask shape: {segmentation_mask.shape}")
print(f"Unique classes found: {np.unique(segmentation_mask)}")

Visualization:
--------------
# PASCAL VOC class names for DeepLabV3 (21 classes including background)
pascal_voc_classes = [
    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',
    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',
    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
]

# Create comprehensive visualization
plt.figure(figsize=(20, 12))

# Original image
plt.subplot(2, 3, 1)
plt.imshow(input_image)
plt.title('Original Image', fontsize=14, fontweight='bold')
plt.axis('off')

# Segmentation mask with custom colormap
plt.subplot(2, 3, 2)
plt.imshow(segmentation_mask, cmap='tab20')
plt.title('Segmentation Mask', fontsize=14, fontweight='bold')
plt.axis('off')

# Overlay segmentation on original image
plt.subplot(2, 3, 3)
overlay = np.array(input_image)
mask_colored = plt.cm.tab20(segmentation_mask / segmentation_mask.max())[:, :, :3]
overlay_alpha = 0.6
blended = overlay_alpha * overlay/255.0 + (1-overlay_alpha) * mask_colored
plt.imshow(blended)
plt.title('Segmentation Overlay', fontsize=14, fontweight='bold')
plt.axis('off')

# Class distribution histogram
plt.subplot(2, 3, 4)
unique_classes, class_counts = np.unique(segmentation_mask, return_counts=True)
class_names = [pascal_voc_classes[i] if i < len(pascal_voc_classes) else f'Class {i}' for i in unique_classes]
plt.bar(range(len(unique_classes)), class_counts, color='skyblue', alpha=0.7)
plt.xticks(range(len(unique_classes)), class_names, rotation=45, ha='right')
plt.title('Segmented Class Distribution', fontsize=14, fontweight='bold')
plt.ylabel('Pixel Count')
plt.grid(True, alpha=0.3)

# Segmentation statistics
plt.subplot(2, 3, 5)
total_pixels = segmentation_mask.size
stats_text = "Segmentation Statistics:\n\n"
for i, (class_idx, count) in enumerate(zip(unique_classes, class_counts)):
    percentage = (count / total_pixels) * 100
    class_name = pascal_voc_classes[class_idx] if class_idx < len(pascal_voc_classes) else f'Class {class_idx}'
    stats_text += f"{class_name}: {percentage:.1f}%\n"

plt.text(0.1, 0.9, stats_text, transform=plt.gca().transAxes, 
         fontsize=10, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
plt.title('Class Coverage Statistics', fontsize=14, fontweight='bold')
plt.axis('off')

# Segmentation quality metrics
plt.subplot(2, 3, 6)
mask_resolution = segmentation_mask.shape
quality_metrics = f"""
Segmentation Quality Metrics:

Image Size: {input_image.size}
Mask Resolution: {mask_resolution}
Classes Detected: {len(unique_classes)}
Total Pixels: {total_pixels:,}
Background Ratio: {(class_counts[0]/total_pixels)*100:.1f}%

Model: DeepLabV3 + MobileNetV2
Dataset: PASCAL VOC 2012
"""

plt.text(0.1, 0.9, quality_metrics, transform=plt.gca().transAxes,
         fontsize=10, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
plt.title('Model Information', fontsize=14, fontweight='bold')
plt.axis('off')

plt.tight_layout()
plt.show()

# Print detailed results
print("\nSegmentation Analysis Complete!")
print("=" * 50)
for class_idx, count in zip(unique_classes, class_counts):
    percentage = (count / total_pixels) * 100
    class_name = pascal_voc_classes[class_idx] if class_idx < len(pascal_voc_classes) else f'Class {class_idx}'
    print(f"{class_name:15}: {count:8,} pixels ({percentage:5.1f}%)")


================================================================================
Experiment 5: Autoencoder for Image Compression on MNIST Dataset
================================================================================

Aim: Design and Implement an Autoencoder Neural Network for Dimensionality Reduction and Image Compression

Theory:
Autoencoders are unsupervised neural networks that learn to compress data into a lower-dimensional latent representation (encoding) and then reconstruct the original input (decoding). The network consists of two main components:

Encoder: Maps high-dimensional input to low-dimensional latent space
Decoder: Reconstructs input from latent representation

Key principles:
- Bottleneck architecture forces learning of compressed representations
- Reconstruction loss (MSE/MAE) drives learning of meaningful features
- Latent space captures essential data characteristics
- Applications include data compression, denoising, anomaly detection, and generative modeling

The compression ratio is determined by the dimensionality reduction between input and latent space. For MNIST (784 dimensions → 32 dimensions), we achieve ~24:1 compression.

Implementation:
---------------
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
import numpy as np

# Load and preprocess MNIST dataset
print("Loading MNIST dataset...")
(x_train, _), (x_test, _) = mnist.load_data()

# Normalize pixel values to [0,1] range
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Reshape to flatten images for fully connected layers
x_train_flat = x_train.reshape(-1, 784)
x_test_flat = x_test.reshape(-1, 784)

print(f"Training data shape: {x_train_flat.shape}")
print(f"Test data shape: {x_test_flat.shape}")

# Define encoder architecture
encoder = models.Sequential([
    layers.Dense(256, activation='relu', input_shape=(784,)),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu')  # Latent space dimension
], name='encoder')

# Define decoder architecture
decoder = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(32,)),
    layers.Dense(128, activation='relu'),
    layers.Dense(256, activation='relu'),
    layers.Dense(784, activation='sigmoid')  # Sigmoid for pixel values [0,1]
], name='decoder')

# Combine encoder and decoder to form complete autoencoder
autoencoder = models.Sequential([encoder, decoder], name='autoencoder')

# Compile with Adam optimizer and mean squared error loss
autoencoder.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

# Display model architecture
autoencoder.summary()

# Train autoencoder (input = target for reconstruction task)
print("Training autoencoder...")
training_history = autoencoder.fit(
    x_train_flat, x_train_flat,
    epochs=20,
    batch_size=256,
    validation_data=(x_test_flat, x_test_flat),
    verbose=1
)

# Evaluate reconstruction performance
test_loss, test_mae = autoencoder.evaluate(x_test_flat, x_test_flat, verbose=0)
print(f"Test Reconstruction Loss (MSE): {test_loss:.6f}")
print(f"Test Mean Absolute Error: {test_mae:.6f}")

Visualization:
--------------
import matplotlib.pyplot as plt

# Plot training history
plt.figure(figsize=(15, 10))

# Loss curves
plt.subplot(2, 3, 1)
plt.plot(training_history.history['loss'], label='Training Loss', linewidth=2)
plt.plot(training_history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.title('Autoencoder Training Loss', fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# MAE curves
plt.subplot(2, 3, 2)
plt.plot(training_history.history['mae'], label='Training MAE', linewidth=2)
plt.plot(training_history.history['val_mae'], label='Validation MAE', linewidth=2)
plt.title('Mean Absolute Error', fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.grid(True, alpha=0.3)

# Original vs Reconstructed images
test_encoded = encoder.predict(x_test_flat[:10])
test_decoded = decoder.predict(test_encoded)

# Display original images
plt.subplot(2, 3, 3)
fig, axes = plt.subplots(2, 5, figsize=(10, 4))
for i in range(5):
    axes[0, i].imshow(x_test[i], cmap='gray')
    axes[0, i].set_title(f'Original {i+1}')
    axes[0, i].axis('off')
    
    axes[1, i].imshow(test_decoded[i].reshape(28, 28), cmap='gray')
    axes[1, i].set_title(f'Reconstructed {i+1}')
    axes[1, i].axis('off')

plt.suptitle('Original vs Reconstructed Images', fontweight='bold')
plt.tight_layout()

# Latent space visualization (2D projection of 32D latent vectors)
plt.subplot(2, 3, 4)
latent_representations = encoder.predict(x_test_flat[:1000])
# Use PCA for 2D visualization
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
latent_2d = pca.fit_transform(latent_representations)

plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=y_test[:1000], cmap='tab10', alpha=0.6)
plt.colorbar(label='Digit Class')
plt.title('Latent Space Visualization (PCA)', fontweight='bold')
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')

# Compression statistics
plt.subplot(2, 3, 5)
original_size = 784
compressed_size = 32
compression_ratio = original_size / compressed_size

stats_text = f"""
Compression Analysis:

Original Size: {original_size} features
Compressed Size: {compressed_size} features
Compression Ratio: {compression_ratio:.1f}:1
Space Saving: {(1 - compressed_size/original_size)*100:.1f}%

Reconstruction Quality:
MSE Loss: {test_loss:.6f}
MAE: {test_mae:.6f}
"""

plt.text(0.1, 0.9, stats_text, transform=plt.gca().transAxes,
         fontsize=10, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))
plt.title('Compression Statistics', fontweight='bold')
plt.axis('off')

# Reconstruction error distribution
plt.subplot(2, 3, 6)
reconstruction_errors = np.mean(np.square(x_test_flat[:1000] - autoencoder.predict(x_test_flat[:1000])), axis=1)
plt.hist(reconstruction_errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Reconstruction Error Distribution', fontweight='bold')
plt.xlabel('MSE per Image')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()


================================================================================
Experiment 6: Denoising Autoencoder for Robust Image Reconstruction
================================================================================

Aim: Build a Denoising Autoencoder to Remove Noise from Corrupted Images and Enhance Robustness

Theory:
Denoising autoencoders extend traditional autoencoders by training on corrupted input data while targeting clean outputs. This approach forces the network to learn robust feature representations that capture essential image characteristics while ignoring noise artifacts.

Key concepts:
- Noise injection: Gaussian noise, salt-and-pepper noise, or dropout corruption
- Robustness: Model learns to ignore irrelevant variations
- Feature learning: Discovers noise-invariant representations
- Regularization effect: Prevents overfitting to training data

Applications include image restoration, preprocessing for computer vision pipelines, and improving model generalization in noisy environments.

Implementation:
---------------
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
import numpy as np

# Load MNIST dataset
print("Loading MNIST dataset for denoising task...")
(x_train, _), (x_test, _) = mnist.load_data()

# Normalize to [0,1] range
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Add channel dimension for convolutional layers
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

print(f"Clean training data shape: {x_train.shape}")

# Generate noisy versions of the data
def add_noise(images, noise_factor=0.4):
    """Add Gaussian noise to images"""
    noisy_images = images + noise_factor * np.random.normal(size=images.shape)
    noisy_images = np.clip(noisy_images, 0.0, 1.0)  # Ensure valid pixel range
    return noisy_images

# Create noisy training and test sets
noise_factor = 0.4
x_train_noisy = add_noise(x_train, noise_factor)
x_test_noisy = add_noise(x_test, noise_factor)

print(f"Noise factor: {noise_factor}")
print(f"Noisy training data shape: {x_train_noisy.shape}")

# Build denoising autoencoder with convolutional layers
input_img = tf.keras.Input(shape=(28, 28, 1))

# Encoder with convolutional layers
encoded = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
encoded = layers.MaxPooling2D((2, 2), padding='same')(encoded)
encoded = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
encoded = layers.MaxPooling2D((2, 2), padding='same')(encoded)
encoded = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
encoded = layers.MaxPooling2D((2, 2), padding='same')(encoded)

# Decoder with upsampling layers
decoded = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)
decoded = layers.UpSampling2D((2, 2))(decoded)
decoded = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoded)
decoded = layers.UpSampling2D((2, 2))(decoded)
decoded = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(decoded)
decoded = layers.UpSampling2D((2, 2))(decoded)
decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(decoded)

# Create and compile denoising autoencoder
denoising_autoencoder = models.Model(input_img, decoded)
denoising_autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['mae'])

# Display architecture
denoising_autoencoder.summary()

# Train with noisy inputs and clean targets
print("Training denoising autoencoder...")
training_history = denoising_autoencoder.fit(
    x_train_noisy, x_train,  # noisy input, clean target
    epochs=15,
    batch_size=128,
    shuffle=True,
    validation_data=(x_test_noisy, x_test),
    verbose=1
)

# Evaluate denoising performance
test_loss, test_mae = denoising_autoencoder.evaluate(x_test_noisy, x_test, verbose=0)
print(f"Denoising Test Loss: {test_loss:.6f}")
print(f"Denoising Test MAE: {test_mae:.6f}")

Visualization:
--------------
import matplotlib.pyplot as plt

# Comprehensive denoising visualization
plt.figure(figsize=(20, 12))

# Training history
plt.subplot(3, 4, 1)
plt.plot(training_history.history['loss'], label='Training Loss', linewidth=2)
plt.plot(training_history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.title('Denoising Training Loss', fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Binary Crossentropy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(3, 4, 2)
plt.plot(training_history.history['mae'], label='Training MAE', linewidth=2)
plt.plot(training_history.history['val_mae'], label='Validation MAE', linewidth=2)
plt.title('Mean Absolute Error', fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.grid(True, alpha=0.3)

# Generate denoised predictions
num_samples = 8
test_indices = np.random.choice(len(x_test), num_samples)
noisy_samples = x_test_noisy[test_indices]
clean_samples = x_test[test_indices]
denoised_samples = denoising_autoencoder.predict(noisy_samples)

# Display comparison: Original, Noisy, Denoised
for i in range(num_samples):
    # Original images
    plt.subplot(3, 8, i + 1)
    plt.imshow(clean_samples[i].reshape(28, 28), cmap='gray')
    plt.title(f'Clean {i+1}', fontsize=10)
    plt.axis('off')
    
    # Noisy images
    plt.subplot(3, 8, i + 9)
    plt.imshow(noisy_samples[i].reshape(28, 28), cmap='gray')
    plt.title(f'Noisy {i+1}', fontsize=10)
    plt.axis('off')
    
    # Denoised images
    plt.subplot(3, 8, i + 17)
    plt.imshow(denoised_samples[i].reshape(28, 28), cmap='gray')
    plt.title(f'Denoised {i+1}', fontsize=10)
    plt.axis('off')

plt.tight_layout()
plt.show()

# Quantitative analysis
plt.figure(figsize=(15, 5))

# PSNR calculation
def calculate_psnr(img1, img2):
    mse = np.mean((img1 - img2) ** 2)
    if mse == 0:
        return 100
    max_pixel = 1.0
    psnr = 20 * np.log10(max_pixel / np.sqrt(mse))
    return psnr

# Calculate PSNR for noisy vs clean and denoised vs clean
psnr_noisy = [calculate_psnr(clean_samples[i], noisy_samples[i]) for i in range(num_samples)]
psnr_denoised = [calculate_psnr(clean_samples[i], denoised_samples[i]) for i in range(num_samples)]

plt.subplot(1, 3, 1)
x_pos = np.arange(num_samples)
plt.bar(x_pos - 0.2, psnr_noisy, 0.4, label='Noisy vs Clean', alpha=0.7)
plt.bar(x_pos + 0.2, psnr_denoised, 0.4, label='Denoised vs Clean', alpha=0.7)
plt.title('PSNR Comparison', fontweight='bold')
plt.xlabel('Sample Index')
plt.ylabel('PSNR (dB)')
plt.legend()
plt.grid(True, alpha=0.3)

# MSE comparison
mse_noisy = [np.mean((clean_samples[i] - noisy_samples[i]) ** 2) for i in range(num_samples)]
mse_denoised = [np.mean((clean_samples[i] - denoised_samples[i]) ** 2) for i in range(num_samples)]

plt.subplot(1, 3, 2)
plt.bar(x_pos - 0.2, mse_noisy, 0.4, label='Noisy vs Clean', alpha=0.7)
plt.bar(x_pos + 0.2, mse_denoised, 0.4, label='Denoised vs Clean', alpha=0.7)
plt.title('MSE Comparison', fontweight='bold')
plt.xlabel('Sample Index')
plt.ylabel('MSE')
plt.legend()
plt.grid(True, alpha=0.3)

# Performance summary
plt.subplot(1, 3, 3)
avg_psnr_noisy = np.mean(psnr_noisy)
avg_psnr_denoised = np.mean(psnr_denoised)
avg_mse_noisy = np.mean(mse_noisy)
avg_mse_denoised = np.mean(mse_denoised)

summary_text = f"""
Denoising Performance Summary:

Noise Factor: {noise_factor}

Average PSNR:
  Noisy: {avg_psnr_noisy:.2f} dB
  Denoised: {avg_psnr_denoised:.2f} dB
  Improvement: {avg_psnr_denoised - avg_psnr_noisy:.2f} dB

Average MSE:
  Noisy: {avg_mse_noisy:.6f}
  Denoised: {avg_mse_denoised:.6f}
  Reduction: {((avg_mse_noisy - avg_mse_denoised)/avg_mse_noisy)*100:.1f}%

Training Loss: {test_loss:.6f}
"""

plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes,
         fontsize=10, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
plt.title('Performance Summary', fontweight='bold')
plt.axis('off')

plt.tight_layout()
plt.show()


================================================================================
Experiment 7: Variational Autoencoder (VAE) for Generative Modeling
================================================================================

Aim: Implement Variational Autoencoder for Learning Probabilistic Latent Representations and Generating New Data

Theory:
Variational Autoencoders (VAEs) extend autoencoders by learning probabilistic latent representations rather than deterministic encodings. VAEs enable both data compression and generation by modeling the latent space as probability distributions.

Key mathematical concepts:
- Encoder outputs mean (μ) and log-variance (log σ²) of latent distribution
- Reparameterization trick: z = μ + σ * ε, where ε ~ N(0,I)
- Loss function: Reconstruction loss + KL divergence regularization
- KL divergence encourages latent variables to follow standard normal distribution

Advantages over standard autoencoders:
- Generative capability through latent space sampling
- Continuous latent space enables interpolation
- Probabilistic framework provides uncertainty quantification
- Better regularization prevents overfitting

Implementation:
---------------
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
import numpy as np

# Load and preprocess MNIST
print("Preparing MNIST data for VAE training...")
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_train = x_train.reshape(-1, 784)
x_test = x_test.reshape(-1, 784)

print(f"Training data shape: {x_train.shape}")
print(f"Test data shape: {x_test.shape}")

# VAE hyperparameters
latent_dim = 2  # 2D latent space for visualization
intermediate_dim = 256
batch_size = 128
epochs = 20

# Sampling layer for reparameterization trick
class Sampling(layers.Layer):
    """Sampling layer using reparameterization trick"""
    
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

# Build encoder
encoder_inputs = tf.keras.Input(shape=(784,))
x = layers.Dense(intermediate_dim, activation='relu')(encoder_inputs)
x = layers.Dense(128, activation='relu')(x)
z_mean = layers.Dense(latent_dim, name='z_mean')(x)
z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)

# Use reparameterization trick to sample from latent distribution
z = Sampling()([z_mean, z_log_var])

# Create encoder model
encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')

# Build decoder
latent_inputs = tf.keras.Input(shape=(latent_dim,))
x = layers.Dense(128, activation='relu')(latent_inputs)
x = layers.Dense(intermediate_dim, activation='relu')(x)
decoder_outputs = layers.Dense(784, activation='sigmoid')(x)

# Create decoder model
decoder = models.Model(latent_inputs, decoder_outputs, name='decoder')

# VAE model class
class VAE(models.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name="reconstruction_loss")
        self.kl_loss_tracker = tf.keras.metrics.Mean(name="kl_loss")

    @property
    def metrics(self):
        return [
            self.total_loss_tracker,
            self.reconstruction_loss_tracker,
            self.kl_loss_tracker,
        ]

    def train_step(self, data):
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)
            
            # Reconstruction loss
            reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(
                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=1
                )
            )
            
            # KL divergence loss
            kl_loss = -0.5 * tf.reduce_mean(
                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)
            )
            
            total_loss = reconstruction_loss + kl_loss

        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        
        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        
        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),
            "kl_loss": self.kl_loss_tracker.result(),
        }

# Create and compile VAE
vae = VAE(encoder, decoder)
vae.compile(optimizer='adam')

print("VAE Architecture:")
print("Encoder:")
encoder.summary()
print("\nDecoder:")
decoder.summary()

# Train VAE
print("Training VAE...")
history = vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))

Visualization:
--------------
import matplotlib.pyplot as plt

# Comprehensive VAE analysis and visualization
plt.figure(figsize=(20, 15))

# Training history
plt.subplot(3, 4, 1)
plt.plot(history.history['loss'], label='Total Loss', linewidth=2)
plt.plot(history.history['reconstruction_loss'], label='Reconstruction Loss', linewidth=2)
plt.plot(history.history['kl_loss'], label='KL Divergence Loss', linewidth=2)
plt.title('VAE Training Losses', fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Latent space visualization (2D)
plt.subplot(3, 4, 2)
z_mean, _, _ = encoder.predict(x_test)
plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test, cmap='tab10', alpha=0.6, s=2)
plt.colorbar(label='Digit Class')
plt.title('Latent Space Distribution', fontweight='bold')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')

# Generated samples from latent space
plt.subplot(3, 4, 3)
n_samples = 15
figure = np.zeros((28 * n_samples, 28 * n_samples))

# Create grid of latent space points
grid_x = np.linspace(-3, 3, n_samples)
grid_y = np.linspace(-3, 3, n_samples)

for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
        z_sample = np.array([[xi, yi]])
        x_decoded = decoder.predict(z_sample)
        digit = x_decoded[0].reshape(28, 28)
        figure[i * 28: (i + 1) * 28, j * 28: (j + 1) * 28] = digit

plt.imshow(figure, cmap='Greys_r')
plt.title('Generated Samples from Latent Space', fontweight='bold')
plt.axis('off')

# Original vs Reconstructed
test_samples = x_test[:10]
reconstructed = vae(test_samples)

for i in range(5):
    # Original
    plt.subplot(3, 10, 4*10 + i + 1)
    plt.imshow(test_samples[i].reshape(28, 28), cmap='gray')
    plt.title(f'Original {i+1}', fontsize=10)
    plt.axis('off')
    
    # Reconstructed
    plt.subplot(3, 10, 4*10 + i + 6)
    plt.imshow(reconstructed[i].numpy().reshape(28, 28), cmap='gray')
    plt.title(f'Reconstructed {i+1}', fontsize=10)
    plt.axis('off')

# Latent space interpolation
plt.subplot(3, 4, 4)
# Select two random points in test set
idx1, idx2 = np.random.choice(len(x_test), 2)
z1_mean, _, _ = encoder.predict(x_test[idx1:idx1+1])
z2_mean, _, _ = encoder.predict(x_test[idx2:idx2+1])

# Interpolation steps
n_steps = 10
interpolation = np.zeros((28, 28 * n_steps))

for i, alpha in enumerate(np.linspace(0, 1, n_steps)):
    z_interp = alpha * z1_mean + (1 - alpha) * z2_mean
    x_interp = decoder.predict(z_interp)
    interpolation[:, i*28:(i+1)*28] = x_interp[0].reshape(28, 28)

plt.imshow(interpolation, cmap='gray')
plt.title('Latent Space Interpolation', fontweight='bold')
plt.axis('off')

plt.tight_layout()
plt.show()

# Additional analysis
plt.figure(figsize=(15, 5))

# Latent space statistics
plt.subplot(1, 3, 1)
z_mean_all, z_log_var_all, _ = encoder.predict(x_test[:1000])
plt.hist2d(z_mean_all[:, 0], z_mean_all[:, 1], bins=50, cmap='Blues')
plt.colorbar(label='Density')
plt.title('Latent Space Density', fontweight='bold')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')

# KL divergence analysis
plt.subplot(1, 3, 2)
kl_per_sample = -0.5 * np.sum(1 + z_log_var_all - np.square(z_mean_all) - np.exp(z_log_var_all), axis=1)
plt.hist(kl_per_sample, bins=50, alpha=0.7, color='orange', edgecolor='black')
plt.title('KL Divergence Distribution', fontweight='bold')
plt.xlabel('KL Divergence per Sample')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# VAE performance summary
plt.subplot(1, 3, 3)
final_total_loss = history.history['loss'][-1]
final_recon_loss = history.history['reconstruction_loss'][-1]
final_kl_loss = history.history['kl_loss'][-1]

summary_text = f"""
VAE Training Summary:

Architecture:
  Latent Dimensions: {latent_dim}
  Intermediate Dims: {intermediate_dim}
  
Final Losses:
  Total Loss: {final_total_loss:.4f}
  Reconstruction: {final_recon_loss:.4f}
  KL Divergence: {final_kl_loss:.4f}
  
Latent Statistics:
  Mean μ range: [{z_mean_all.min():.2f}, {z_mean_all.max():.2f}]
  Log-var range: [{z_log_var_all.min():.2f}, {z_log_var_all.max():.2f}]
  
Training Parameters:
  Epochs: {epochs}
  Batch Size: {batch_size}
"""

plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes,
         fontsize=10, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.8))
plt.title('VAE Performance Summary', fontweight='bold')
plt.axis('off')

plt.tight_layout()
plt.show()


================================================================================
Experiment 8: Convolutional Neural Network (CNN) for MNIST Digit Classification
================================================================================

Aim: Design and Implement a Convolutional Neural Network for Handwritten Digit Recognition with Comprehensive Performance Analysis

Theory:
Convolutional Neural Networks (CNNs) are specialized deep learning architectures designed for processing grid-like data such as images. CNNs leverage spatial locality and translation invariance through convolutional operations, making them highly effective for computer vision tasks.

Key CNN components:
- Convolutional layers: Apply learnable filters to extract spatial features
- Pooling layers: Reduce spatial dimensions while preserving important features
- Activation functions: Introduce non-linearity (commonly ReLU)
- Fully connected layers: Perform final classification based on extracted features

CNN advantages for image classification:
- Translation invariance: Recognizes patterns regardless of location
- Parameter sharing: Reduces overfitting compared to fully connected networks
- Hierarchical feature learning: Low-level to high-level feature extraction
- Spatial locality: Captures local correlations in images

MNIST dataset challenges: 28x28 grayscale images of handwritten digits (0-9) with 60,000 training and 10,000 test samples.

Implementation:
---------------
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
import numpy as np

# Load and preprocess MNIST dataset
print("Loading MNIST dataset for CNN classification...")
(x_train, y_train), (x_test, y_test) = mnist.load_data()

print(f"Original training data shape: {x_train.shape}")
print(f"Original test data shape: {x_test.shape}")
print(f"Number of classes: {len(np.unique(y_train))}")

# Reshape data to add channel dimension and normalize
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0

# Convert labels to one-hot encoded format
y_train_categorical = to_categorical(y_train, 10)
y_test_categorical = to_categorical(y_test, 10)

print(f"Preprocessed training data shape: {x_train.shape}")
print(f"Preprocessed test data shape: {x_test.shape}")
print(f"One-hot labels shape: {y_train_categorical.shape}")

# Build CNN architecture
cnn_model = models.Sequential([
    # First convolutional block
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Second convolutional block
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Third convolutional block
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.25),
    
    # Flatten and fully connected layers
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
], name='MNIST_CNN')

# Compile model with advanced optimization
cnn_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy', 'top_2_accuracy']
)

# Display model architecture
print("\nCNN Model Architecture:")
cnn_model.summary()

# Implement learning rate scheduling
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_accuracy',
    factor=0.5,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

# Early stopping to prevent overfitting
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_accuracy',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

# Train CNN with callbacks
print("Training CNN...")
training_history = cnn_model.fit(
    x_train, y_train_categorical,
    batch_size=128,
    epochs=20,
    validation_split=0.1,
    callbacks=[lr_scheduler, early_stopping],
    verbose=1
)

# Comprehensive model evaluation
print("\nEvaluating CNN performance...")
test_results = cnn_model.evaluate(x_test, y_test_categorical, verbose=0)
print(f"Test Loss: {test_results[0]:.6f}")
print(f"Test Accuracy: {test_results[1]*100:.2f}%")
print(f"Test Top-2 Accuracy: {test_results[2]*100:.2f}%")

Visualization:
--------------
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Comprehensive CNN analysis and visualization
plt.figure(figsize=(20, 15))

# Training history visualization
plt.subplot(4, 4, 1)
plt.plot(training_history.history['accuracy'], label='Training Accuracy', linewidth=2)
plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
plt.title('Model Accuracy', fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(4, 4, 2)
plt.plot(training_history.history['loss'], label='Training Loss', linewidth=2)
plt.plot(training_history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.title('Model Loss', fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(4, 4, 3)
if 'lr' in training_history.history:
    plt.plot(training_history.history['lr'], linewidth=2, color='red')
    plt.title('Learning Rate Schedule', fontweight='bold')
    plt.xlabel('Epoch')
    plt.ylabel('Learning Rate')
    plt.yscale('log')
    plt.grid(True, alpha=0.3)

# Generate predictions for analysis
y_pred_proba = cnn_model.predict(x_test)
y_pred = np.argmax(y_pred_proba, axis=1)

# Confusion Matrix
plt.subplot(4, 4, 4)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix', fontweight='bold')
plt.xlabel('Predicted')
plt.ylabel('Actual')

# Sample predictions visualization
sample_indices = np.random.choice(len(x_test), 8)
for i, idx in enumerate(sample_indices):
    plt.subplot(4, 8, 17 + i)
    plt.imshow(x_test[idx].reshape(28, 28), cmap='gray')
    plt.title(f'True: {y_test[idx]}, Pred: {y_pred[idx]}', fontsize=10)
    plt.axis('off')
    
    # Add confidence score
    confidence = np.max(y_pred_proba[idx])
    color = 'green' if y_test[idx] == y_pred[idx] else 'red'
    plt.text(14, 30, f'{confidence:.3f}', ha='center', color=color, fontweight='bold')

# Class-wise accuracy analysis
plt.subplot(4, 4, 9)
class_accuracy = []
for digit in range(10):
    digit_indices = (y_test == digit)
    digit_accuracy = np.mean(y_pred[digit_indices] == digit)
    class_accuracy.append(digit_accuracy)

plt.bar(range(10), class_accuracy, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Per-Class Accuracy', fontweight='bold')
plt.xlabel('Digit Class')
plt.ylabel('Accuracy')
plt.ylim(0.9, 1.0)
for i, acc in enumerate(class_accuracy):
    plt.text(i, acc + 0.001, f'{acc:.3f}', ha='center', fontweight='bold')
plt.grid(True, alpha=0.3)

# Feature map visualization (first conv layer)
plt.subplot(4, 4, 10)
# Get intermediate model to extract feature maps
feature_extractor = models.Model(inputs=cnn_model.input,
                                outputs=cnn_model.layers[0].output)
sample_image = x_test[0:1]
feature_maps = feature_extractor(sample_image)

# Display first 16 feature maps
fig, axes = plt.subplots(4, 4, figsize=(6, 6))
for i in range(16):
    ax = axes[i//4, i%4]
    ax.imshow(feature_maps[0, :, :, i], cmap='viridis')
    ax.set_title(f'Filter {i+1}', fontsize=8)
    ax.axis('off')
plt.suptitle('First Layer Feature Maps', fontweight='bold')

# Prediction confidence distribution
plt.subplot(4, 4, 11)
confidences = np.max(y_pred_proba, axis=1)
plt.hist(confidences, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')
plt.title('Prediction Confidence Distribution', fontweight='bold')
plt.xlabel('Maximum Softmax Probability')
plt.ylabel('Frequency')
plt.grid(True, alpha=0.3)

# Error analysis
plt.subplot(4, 4, 12)
errors = (y_test != y_pred)
error_rate_per_confidence = []
confidence_bins = np.linspace(0.5, 1.0, 11)

for i in range(len(confidence_bins)-1):
    mask = (confidences >= confidence_bins[i]) & (confidences < confidence_bins[i+1])
    if np.sum(mask) > 0:
        error_rate = np.mean(errors[mask])
        error_rate_per_confidence.append(error_rate)
    else:
        error_rate_per_confidence.append(0)

plt.plot(confidence_bins[:-1], error_rate_per_confidence, 'o-', linewidth=2, markersize=6)
plt.title('Error Rate vs Confidence', fontweight='bold')
plt.xlabel('Confidence Bin')
plt.ylabel('Error Rate')
plt.grid(True, alpha=0.3)

# Model performance summary
plt.subplot(4, 4, 13)
final_accuracy = training_history.history['val_accuracy'][-1]
final_loss = training_history.history['val_loss'][-1]
total_params = cnn_model.count_params()
trainable_params = sum([tf.keras.backend.count_params(w) for w in cnn_model.trainable_weights])

summary_text = f"""
CNN Performance Summary:

Architecture:
  Total Parameters: {total_params:,}
  Trainable Parameters: {trainable_params:,}
  
Training Results:
  Final Validation Accuracy: {final_accuracy*100:.2f}%
  Final Validation Loss: {final_loss:.4f}
  Training Epochs: {len(training_history.history['loss'])}
  
Test Performance:
  Test Accuracy: {test_results[1]*100:.2f}%
  Test Loss: {test_results[0]:.4f}
  Top-2 Accuracy: {test_results[2]*100:.2f}%
  
Model Complexity:
  Input Shape: (28, 28, 1)
  Conv Layers: 3
  Dense Layers: 2
  Dropout Rate: 0.25-0.5
"""

plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes,
         fontsize=9, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
plt.title('Performance Summary', fontweight='bold')
plt.axis('off')

# Misclassified examples
plt.subplot(4, 4, 14)
misclassified_indices = np.where(y_test != y_pred)[0][:8]
fig, axes = plt.subplots(2, 4, figsize=(8, 4))
for i, idx in enumerate(misclassified_indices):
    ax = axes[i//4, i%4]
    ax.imshow(x_test[idx].reshape(28, 28), cmap='gray')
    ax.set_title(f'T:{y_test[idx]} P:{y_pred[idx]} C:{np.max(y_pred_proba[idx]):.2f}', 
                fontsize=8)
    ax.axis('off')
plt.suptitle('Misclassified Examples', fontweight='bold')

plt.tight_layout()
plt.show()

# Print detailed classification report
print("\nDetailed Classification Report:")
print("=" * 60)
print(classification_report(y_test, y_pred, target_names=[str(i) for i in range(10)]))

# Calculate and display additional metrics
accuracy = np.mean(y_test == y_pred)
error_rate = 1 - accuracy
avg_confidence = np.mean(confidences)
print(f"\nAdditional Metrics:")
print(f"Overall Accuracy: {accuracy*100:.2f}%")
print(f"Error Rate: {error_rate*100:.2f}%")
print(f"Average Prediction Confidence: {avg_confidence:.4f}")
print(f"Total Misclassifications: {np.sum(errors)}/{len(y_test)}")
